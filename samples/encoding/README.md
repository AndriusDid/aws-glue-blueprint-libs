# README: Blueprint - Converting character encoding

## Overview

Unicode has become the standard in modern systems. However, a lot of customers who use CJK (Chinese-Japanese-Korean) characters are still struggling with handling those character codes. From the viewpoint of ETL, it is important to convert those characters into unicode.
With this blueprint, you can convert your non-UTF files into UTF encoded files.

This blueprint has been tested with the following conditions, and it will work for other character encodings as well. 
We recommend that you test and modify the script based on your actual use case.

* Glue version: Glue 2.0 (Spark 2.4.3)
* Character encoding:
    * Simplified Chinese: gb2312 (gb2312)
    * Traditional Chinese: Big5 (big5)
    * Japanese: Shift-JIS (sjis)
    * Korean: EUC-KR (euc-kr)

## Resources

```
encoding/
```

## How to use it

### Input/Output

* Input
    * Input files encoded with non UTF-8
* Output
    * Output files encoded with UTF-8

### Parameters

* WorkflowName: Name for the workflow.
* IAMRole: IAM role used for the generated crawlers and jobs.
* InputDataLocation: Input data location (Amazon S3 path). Data is read from this location.
* InputDataFormat: Input file format. 
    * Valid values are '`csv`', '`json`', '`parquet`', '`orc`', and '`avro`'.
* InputDataEncoding: Provide an encoding string. 
    * For example: '`sjis`', '`euc-kr`', '`big5`', '`gb2312`'
* OutputDataLocation: Output data location (Amazon S3 path). Data is written into this location.
* (Optional) PartitionKeys: Comma-separated column names that you use as partition keys. 
    * If you do not provide partition keys, the data will be written into tables directly without any partitions. 
* NumberOfWorkers: The number of G.1X workers in the AWS Glue job.

### Considerations

* This blueprint is for a single source s3 path.
* Whether there are multiple files or multiple folders under the source, they should all have the same schema.
* Converted data will be written to a different S3 location.
* Data in the destination is overwritten.

### Limitations

* A workflow generated by this blueprint is designed to be run once. For new data, create a new workflow with new paths. If you run the same workflow again, then partial data may be visible during the course of a workflow run as overwrite the data in destination. 
* Amazon S3 paths provided for InputDataLocation should be different from OutputDataLocation.

## Tutorial

1. Download the files
2. Compress the blueprint files into a zip archive.
    
    $ zip encoding.zip encoding/*
3. Upload `encoding.zip` to an Amazon S3 bucket.
    
    $ aws s3 cp encoding.zip s3://path/to/blueprint/
4. Sign in to the AWS Glue console, and in the navigation pane, choose **Blueprints**.
5. Choose **Add blueprint**.
6. Specify `encoding-tutorial` in **Blueprint name** and `s3://path/to/blueprint/encoding.zip` in **ZIP archive location (S3).**, and then choose **Add blueprint**.
7. Wait for the blueprint to be **ACTIVE**.
8. Select your `encoding-tutorial` blueprint, and choose **Create workflow** from the **Actions** menu.
9. Specify parameters and choose **Submit.**
    1. WorkflowName: `encoding`
    2. IAMRole: `GlueServiceRole`
    3. InputDataLocation: `s3://path/to/input/location_sjis/`
    4. InputDataFormat: `json`
    5. InputDataEncoding: `sjis`
    6. OutputDataLocation: `s3://path/to/output/data/location/`
    7. PartitionKeys: `year`, `month`, `day` (Use your partition keys)
    8. NumberOfWorkers: `5` (Use default value)
    9. IAM role: `GlueServiceRole`
        
        Note: This role is used to create the entities in the workflow.
10. Wait for the blueprint run to be **SUCCEEDED**.
11. Choose **Workflows** in the navigation pane.
12. Select the `encoding` workflow and choose **Run** from the **Actions** menu.